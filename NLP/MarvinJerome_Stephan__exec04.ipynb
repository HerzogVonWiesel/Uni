{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5811d32a",
      "metadata": {
        "id": "5811d32a"
      },
      "source": [
        "# Exercise \"Natural Language Processing\" -- Text Classification with Huggingface\n",
        "\n",
        "For this course, save a COPY to your Google Drive for the tutorial (File -> Save copy in Drive). Then complete the tasks in your saved copy. If you're done, submit your notebook with the name `{yourFirstName_yourLastName}.ipynb` via moodle **by sharing a link** with the full (i.e. read + write) permissions. We will do the grading in the Colab Notebook copy. Please avoid uploading `ipynb` files and rather, share the link to your notebook with us.\n",
        "\n",
        "This is an individual assignment, i.e., submit your solutions individually.\n",
        "\n",
        "This assignment is **mandatory for participation in the exam**. You are required to obtain at least 50% of the total points in this assignment to become eligible for participating in the final exam.\n",
        "\n",
        "\n",
        "**Due date: 15.06.2023, 9:15 a.m.(CEST)**\n",
        "\n",
        "----\n",
        "\n",
        "In this assignment we will revisit the text classification task from the previous assignment but we will solve it using the Transformer model architecture implemented with the HuggingFace `transformers` library.\n",
        "\n",
        "In particular, we will:\n",
        "- Take a look at the Transformer model architecture\n",
        "- Define data loading and processing pipelines for the `AG_NEWS` dataset\n",
        "- Train a BERT-style Transformer model for classifiying the news articles in `AG_NEWS`\n",
        "- Use Transfer Learning to boost performance on the text classification task\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "tr24ph1Cc612",
      "metadata": {
        "id": "tr24ph1Cc612"
      },
      "source": [
        "First, we need to install a few packages and we are ready to go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ce65360b",
      "metadata": {
        "id": "ce65360b"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets evaluate accelerate scikit-learn"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "EwH_P31pG-Za",
      "metadata": {
        "id": "EwH_P31pG-Za"
      },
      "source": [
        "## Background on Transformer Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "rPPNjtS93iqr",
      "metadata": {
        "id": "rPPNjtS93iqr"
      },
      "source": [
        "In this assignment, we will use the Transformer neural network architecture. This architecture is much more powerful than the simple MLP from last assignment!\n",
        "\n",
        "\n",
        "You might find different styles of Transformer models online. For this assignment, we will use BERT-style Transformer models (sometimes also called \"encoder-only\" Transformers). \n",
        "\n",
        "Developing an intuitive understanding of how Transformers work is important but not trivial. Besides the lecture content, we recommend this excellent write-up for Transformers: https://jalammar.github.io/illustrated-transformer/."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "N7wNSk9h-fH2",
      "metadata": {
        "id": "N7wNSk9h-fH2"
      },
      "source": [
        "Now, let's take a look at single Transformer model. We will use the `transformers` library by HuggingFace, which makes training Transformer models really easy and convenient (and is used by many researchers around the world)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "F2Ytu4IA-0aW",
      "metadata": {
        "id": "F2Ytu4IA-0aW"
      },
      "source": [
        "**Task 1 (1 point)**: Load and instantiate the `distilroberta-base` model with `transformers`; then print the model object using Python's native `print` function. \n",
        "\n",
        "`distilroberta-base` is a BERT-style Transformer model derived from the RoBERTa architecture (Robustly optimized BERT Pretraining approach). As you can see, NLP researchers like puns. It has been \"distilled\", e.g. approximated using a smaller model size. We are using it so that training using Colab's single GPU is faster, since the model is smaller than the original variants.\n",
        "\n",
        "HINT: `transformers.AutoModel.from_pretrained()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "TJzjnwpN-zNf",
      "metadata": {
        "id": "TJzjnwpN-zNf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RobertaModel(\n",
            "  (embeddings): RobertaEmbeddings(\n",
            "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "    (token_type_embeddings): Embedding(1, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): RobertaEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0-5): 6 x RobertaLayer(\n",
            "        (attention): RobertaAttention(\n",
            "          (self): RobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): RobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): RobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): RobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pooler): RobertaPooler(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel # hint\n",
        "\n",
        "model = AutoModel.from_pretrained(\"distilroberta-base\")\n",
        "print(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ptzUhoiC_czU",
      "metadata": {
        "id": "ptzUhoiC_czU"
      },
      "source": [
        "The `print` statement should have given you a nice overview over the entire RoBERTa Transformer model. You should be able to identify the layers that are responsible for the attention mechanism. RoBERTa is a highly optimized model architecture (although nowadays even better architectures exist). That is why there are many different modules, it is okay if you do not understand the role of every last one for now.\n",
        "\n",
        "In the `embedding` module of `distilroberta-base`, you should also be able to see a module called `position_embeddings`.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "dxjDaSvrNcOL",
      "metadata": {
        "id": "dxjDaSvrNcOL"
      },
      "source": [
        "\n",
        "**Task 2 (2 points):** What is the role of the positional embedding in BERT-style Transformer models. Why is it necessary? (roughly 2-4 sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "P3gHp5Gb_uHh",
      "metadata": {
        "id": "P3gHp5Gb_uHh"
      },
      "outputs": [],
      "source": [
        "# What is the role of the positional embedding in BERT-style Transformer models. Why is it necessary? (roughly 2-4 sentences)# Text answer below\n",
        "\n",
        "# The positional embedding is necessary because the model does not understand the order of words. It is a vector that is added to the word embedding to give the model information about the (relative) position of the word in the sentence.\n",
        "# This is necessary because the same word can have different meanings depending on its position in the sentence; e.g. \"The leaves fall in fall\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "KAoSBP8xhI-0",
      "metadata": {
        "id": "KAoSBP8xhI-0"
      },
      "source": [
        "**Task 3 (2 points)**: What is the role of the attention mechanism in BERT-style Transformer models? (roughly 2-4 sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "U5IJGZ84hvKV",
      "metadata": {
        "id": "U5IJGZ84hvKV"
      },
      "outputs": [],
      "source": [
        "# What is the role of the attention mechanism in BERT-style Transformer models? (roughly 2-4 sentences)\n",
        "# Text answer below\n",
        "\n",
        "# The attention mechanism in a model gives contextual information to each word by looking other words in the sentence. It gives more weight to words that are more relevant to the word it is looking at; e.g. \"The apple tastes good because it is ripe.\" The model will know that \"it\" refers to \"apple\" because of the attention mechanism."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2e_Cv0HGGiJW",
      "metadata": {
        "id": "2e_Cv0HGGiJW"
      },
      "source": [
        "## Data Pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "g-YUfYI3HDG5",
      "metadata": {
        "id": "g-YUfYI3HDG5"
      },
      "source": [
        "Now, let's prepare for training our own Transformer model. As always, we need to set up a data processing pipeline."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "42dc8b44",
      "metadata": {
        "id": "42dc8b44"
      },
      "source": [
        "We will still use the `AG_NEWS` dataset. Last time, we used `torch.datasets` to load `AG_NEWS`. Besides `transformers`, HuggingFace also has a library called `datasets` that is widely used and supports most popular (and even unpopular) datasets. So this time, we will use `datasets` to load `AG_NEWS`!\n",
        "\n",
        "\n",
        "**Task 4 (3 points)**: \n",
        "- Load the `AG_NEWS` dataset using HuggingFace `datasets`.\n",
        "- Transformer models need a lot of compute and using the entire dataset will take too much of your time for this assignment. Shuffle the train split of the dataset and select only the first `4000` samples after shuffling; discard the rest.\n",
        "- We still need to create a dev split. Take a random sample of 10% of the samples from the (reduced) train set (hint: `datasets.train_test_split`). For compatibility with `transformers`, the dev split should be assigned under the `\"val\"` keyword (for validation). Background: validation and dev split are two names for the same thing. There is a fierce debate between ML researchers over which one is correct.\n",
        "\n",
        "Use a fixed random seed of 42 whenever random number generation is involved to ensure reproducibility. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "3_y69lpFauHc",
      "metadata": {
        "id": "3_y69lpFauHc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset ag_news (/home/jerome/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
            "100%|██████████| 2/2 [00:00<00:00, 1127.05it/s]\n",
            "Loading cached shuffled indices for dataset at /home/jerome/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-dd0ff9596fea92b0.arrow\n",
            "Loading cached shuffled indices for dataset at /home/jerome/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-12f3c4e4bf422cce.arrow\n",
            "Loading cached split indices for dataset at /home/jerome/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-b0522893c60a5a50.arrow and /home/jerome/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-291482d2afab481f.arrow\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset  # hint\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# Wow, we already did it for you!\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "dataset = dataset.shuffle(SEED)\n",
        "\n",
        "# TODO: select the first 4000 samples from the training set to make the dataset manageable for this assignment\n",
        "\n",
        "dataset[\"train\"] = dataset[\"train\"].select(range(4000))\n",
        "\n",
        "\n",
        "assert len(dataset[\"train\"]) == 4000, f'you should have 4000 samples but you have {len(dataset[\"train\"])} samples'\n",
        "\n",
        "# TODO: generate the validation set from the training data.\n",
        "# Assume the training set was not shuffled, perform a shuffle before splitting. This is good practice, so we practice it here as well.\n",
        "# hint: you can use the train_test_split method from HF. Remember the seed!\n",
        "\n",
        "temp = dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True, seed=SEED)\n",
        "dataset[\"train\"] = temp[\"train\"]\n",
        "dataset[\"val\"] = temp[\"test\"]\n",
        "\n",
        "assert len(dataset[\"test\"]) == 7600, f'test set should not be touched but you have {len(dataset[\"test\"])} samples'\n",
        "assert len(dataset[\"train\"]) == 3600, f'train set should be 3600 samples but you have {len(dataset[\"train\"])} samples'\n",
        "assert len(dataset[\"val\"]) == 400, f'val set should be 400 samples but you have {len(dataset[\"val\"])} samples'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "98rRHvkZFt7T",
      "metadata": {
        "id": "98rRHvkZFt7T"
      },
      "source": [
        "**Task 5 (2 points):** For each split, calculate the distribution of class labels and print them (e.g. train: 0 ->  0.23 % | 1 ->  0.27 % | 2 ->  0.24 % | 3 ->  0.26%). Are the labels (roughly) balanced? What could we do to guarantee a balanced random split?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "509949b6",
      "metadata": {
        "id": "509949b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train\n",
            "0 -> 25.36 %\n",
            "1 -> 25.67 %\n",
            "2 -> 23.19 %\n",
            "3 -> 25.78 %\n",
            "val\n",
            "0 -> 23.50 %\n",
            "1 -> 19.00 %\n",
            "2 -> 26.75 %\n",
            "3 -> 30.75 %\n",
            "test\n",
            "0 -> 25.00 %\n",
            "1 -> 25.00 %\n",
            "2 -> 25.00 %\n",
            "3 -> 25.00 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# TODO: for each split, calculate the class label distribution and print it\n",
        "\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    print(split)\n",
        "    for label_index in range(4):\n",
        "        print(f\"{label_index} -> {np.sum(np.array(dataset[split]['label']) == label_index) / len(dataset[split]['label']) * 100:.2f} %\")\n",
        "\n",
        "\n",
        "\n",
        "# EXTRA TODO: store the number of class labels in `num_labels` (we will use it later)\n",
        "num_labels: int = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "d5CEsi8tdHnP",
      "metadata": {
        "id": "d5CEsi8tdHnP"
      },
      "outputs": [],
      "source": [
        "# Are the labels (roughly) balanced? What could we do to guarantee a balanced random split?\n",
        "\n",
        "# The labels are roughly balanced for the train and test set, but not for the validation set. We could use stratified sampling to guarantee a balanced random split (stratify_by_column for the HF train_test_split method)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6a3cdc5d",
      "metadata": {
        "id": "6a3cdc5d"
      },
      "source": [
        "**Task 6 (7 points)**: \n",
        "\n",
        "Similarly to the assignment-3, write a *preprocess_function* which:\n",
        "- tokenizes the text with the DistilRoBERTa tokenizer (hint: `transformers.AutoTokenizer.from_pretrained()`)\n",
        "- truncates the result to a maximum of max_sequence_length tokens, if longer\n",
        "- pads the result to max_sequence_length using the padding token from DistilRoBERTa\n",
        "- converts all tokens into token IDs (expected output: list of integers)\n",
        "\n",
        "HINT: You can implement all these things manually, but they can also be done in a single line using the tokenizer implementation from HuggingFace.\n",
        "\n",
        "Finally call this function for each sample of your train, test and validation set using the `map` method from the HuggingFace `datasets` library.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "dDiFd3Iudukh",
      "metadata": {
        "id": "dDiFd3Iudukh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /home/jerome/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-8a8c13ff1774fe94.arrow\n",
            "Loading cached processed dataset at /home/jerome/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-8c2a838058f67ead.arrow\n",
            "Loading cached processed dataset at /home/jerome/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-1d99c0de6612b0e0.arrow\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer  # hint\n",
        "from typing import Dict, Any\n",
        "from datasets import Dataset\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
        "\n",
        "\n",
        "# TODO: define a preprocessing function to tokenize a sample\n",
        "def preprocess_function(sample: Dict[str, Any], seq_len: int):\n",
        "    \"\"\"\n",
        "    Function applied to all the examples in the Dataset (individually or in batches). \n",
        "    It accepts as input a sample as a dictionary and return a new dictionary with the BERT tokens for that sample\n",
        "\n",
        "    Args:\n",
        "        sample Dict[str, Any]:\n",
        "            Dictionary of sample\n",
        "            \n",
        "    Returns:\n",
        "        Dict: Dictionary of tokenized sample in the following style:\n",
        "        {\n",
        "          \"input_ids\": list[int] # token ids\n",
        "          \"attention_mask\": list[int] # Mask for self-attention (padding tokens are ignored).\n",
        "        }\n",
        "        Hint: if your are using the Huggingface tokenizer implementation, this is the default output format but check it yourself to be sure!\n",
        "    \"\"\"\n",
        "    Dict = tokenizer(sample[\"text\"], truncation=True, padding=\"max_length\", max_length=seq_len)\n",
        "    return Dict\n",
        "\n",
        "\n",
        "# TEST for truncation\n",
        "mock_example_long = Dataset.from_list([{\"text\": (\"lorem ipsum dolar sonet \" * 10_000) }]).map(\n",
        "    preprocess_function, batched=True, fn_kwargs={\"seq_len\": 256}\n",
        ")\n",
        "assert len(mock_example_long[0][\"input_ids\"]) == 256\n",
        "\n",
        "# TEST for padding\n",
        "mock_example_short = Dataset.from_list([{\"text\": (\"lorem ipsum dolar sonet \" * 2) }]).map(\n",
        "    preprocess_function, batched=True, fn_kwargs={\"seq_len\": 256}\n",
        ")\n",
        "assert len(mock_example_short[0][\"input_ids\"]) == 256\n",
        "assert mock_example_short[0][\"input_ids\"][-1] == tokenizer.pad_token_id\n",
        "\n",
        "# TODO: use the `map` function to tokenize your dataset. store the results in `encoded_ds`\n",
        "encoded_ds = dataset.map(preprocess_function, batched=True, fn_kwargs={\"seq_len\": 256})\n",
        "\n",
        "\n",
        "# TEST  dataset\n",
        "assert len(encoded_ds[\"train\"][0][\"input_ids\"]) == 256\n",
        "assert len(encoded_ds[\"val\"][0][\"input_ids\"]) == 256\n",
        "assert len(encoded_ds[\"test\"][0][\"input_ids\"]) == 256\n",
        "assert len(encoded_ds[\"train\"][50][\"input_ids\"]) == 256"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "640a8dac",
      "metadata": {
        "id": "640a8dac"
      },
      "source": [
        "**Task 7 (3 points)**: Define a function to evaluate the model during training. The function is automatically called by the trainer on the validation or test set, it must take an `EvalPrediction` object \n",
        "(see https://huggingface.co/docs/transformers/main/en/internal/trainer_utils#transformers.EvalPrediction) \n",
        "and return a dictionary `dict[str, float]` mapping metric names (`str`) to metric values (`float`).\n",
        "\n",
        "\n",
        "HINT: Again, HuggingFace has a convenient library for evaluation called `evaluate`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "ucdfL-Zsz4Bl",
      "metadata": {
        "id": "ucdfL-Zsz4Bl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jerome/anaconda3/envs/umlia23/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/jerome/anaconda3/envs/umlia23/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import evaluate # hint\n",
        "import numpy as np\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "\n",
        "# TODO\n",
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "    \"\"\"\n",
        "    The function that will be used to compute metrics at evaluation. Must take a EvalPrediction and return a dictionary string to metric values.\n",
        "\n",
        "    Args:\n",
        "        eval_pred EvalPrediction:\n",
        "            Evaluation output (always contains labels), to be used to compute metrics.\n",
        "            It has one Numpy array with predictions and one with labels.\n",
        "            \n",
        "    Returns:\n",
        "        Dict: Dictionary of metric values\n",
        "    \"\"\"\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    labels = eval_pred.label_ids\n",
        "    precision_metric = evaluate.load(\"precision\")\n",
        "    recall_metric = evaluate.load(\"recall\")\n",
        "    f1_metric = evaluate.load(\"f1\")\n",
        "    accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    Dict = {}\n",
        "    Dict.update(precision)\n",
        "    Dict.update(recall)\n",
        "    Dict.update(f1)\n",
        "    Dict.update(accuracy)\n",
        "    return Dict\n",
        "\n",
        "\n",
        "# TEST\n",
        "predictions = np.array([[0.7, 0.3, 0.9]])\n",
        "labels = np.array([2])\n",
        "eval_pred = EvalPrediction(predictions=predictions, label_ids=labels)\n",
        "assert compute_metrics(eval_pred)[\"accuracy\"] == 1\n",
        "\n",
        "predictions = np.array([[0.7, 0.3, 0.9], [0.9, 0.1, 0.1]])\n",
        "labels = np.array([2, 1])\n",
        "eval_pred = EvalPrediction(predictions=predictions, label_ids=labels)\n",
        "assert compute_metrics(eval_pred)[\"accuracy\"] == 0.5"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c0f68aa1",
      "metadata": {
        "id": "c0f68aa1"
      },
      "source": [
        "# Training from scratch\n",
        "\n",
        "Now, we will start training our first Transformer model. We will begin by training a model **from scratch**, e.g. from randomly initialized weights.\n",
        "\n",
        "In the last assignment, we coded the entire training loop from scratch as well. In practice, it is often easier to use pre-implemented `Trainer` classes instead. Suprise: Huggingface provides such a `Trainer` implementation. \n",
        "\n",
        "The Huggingface `Trainer` abstracts away a lot of the complexity of training models! But to really understand what's going on and debug errors, it is crucial to know what's happening under the hood. \n",
        "\n",
        "**Task 8 (10 points)**: \n",
        "- Initialize the `distilroberta-base` model from HuggingFace **with random weights** and train it from scratch for 5 epochs. Use the `Trainer` class from HuggingFace for the training loop implementation and `AutoModelForSequenceClassification` instead of `AutoModel` to load the model. HINT: You definitely want to train on GPU this time, otherwise it will be very slow. \n",
        "\n",
        "* Why is it important to use `AutoModelForSequenceClassification` instead of `AutoModel`? How is the Transformer model architecture modified to predict the class labels? Describe in 3-6 sentences (rough guideline). HINT: You will need to resarch a bit on your own for this.\n",
        "\n",
        "\n",
        "You do not have to reach a specific perofrmance goal for this task. It is rather about building an understanding of how to work with Transformer models. An accuracy of below 60% after 5 epochs means things are probably not working as intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "5H8YQM0xz2iM",
      "metadata": {
        "id": "5H8YQM0xz2iM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jerome/anaconda3/envs/umlia23/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='565' max='565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [565/565 01:35, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.386652</td>\n",
              "      <td>0.297859</td>\n",
              "      <td>0.257979</td>\n",
              "      <td>0.095802</td>\n",
              "      <td>0.197500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.306614</td>\n",
              "      <td>0.534229</td>\n",
              "      <td>0.425887</td>\n",
              "      <td>0.335615</td>\n",
              "      <td>0.427500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.936076</td>\n",
              "      <td>0.625624</td>\n",
              "      <td>0.641894</td>\n",
              "      <td>0.626027</td>\n",
              "      <td>0.622500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.830387</td>\n",
              "      <td>0.698413</td>\n",
              "      <td>0.670678</td>\n",
              "      <td>0.673123</td>\n",
              "      <td>0.665000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.147100</td>\n",
              "      <td>0.791492</td>\n",
              "      <td>0.692867</td>\n",
              "      <td>0.704645</td>\n",
              "      <td>0.696616</td>\n",
              "      <td>0.690000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jerome/anaconda3/envs/umlia23/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.7914922833442688,\n",
              " 'eval_precision': 0.692866589083433,\n",
              " 'eval_recall': 0.7046449484730115,\n",
              " 'eval_f1': 0.6966160073789975,\n",
              " 'eval_accuracy': 0.69,\n",
              " 'eval_runtime': 7.9519,\n",
              " 'eval_samples_per_second': 50.303,\n",
              " 'eval_steps_per_second': 1.635,\n",
              " 'epoch': 5.0}"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoConfig  # hint\n",
        "from transformers import TrainingArguments, Trainer  # hint\n",
        "\n",
        "# TODO: Create `TrainingArguments`. You can mostly use default values, but setting `per_device_train_batch_size` and `per_device_eval_batch_size` to 32 and a learning rate of 2e-5 worked well for us.\n",
        "# Set arguments to do the following: Evaluate and save a checkpoint every epoch. At the end of training, load the weights of the best checkpoint (measured by loss on the validation set). Set the seed to 1944 (Hasso's birthyear).\n",
        "# Don't forget to set the number of training epochs!\n",
        "\n",
        "TrainingArguments = TrainingArguments(\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-5,\n",
        "    output_dir=\"./results_scratch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    seed=1944,\n",
        "    num_train_epochs=5,\n",
        ")\n",
        "\n",
        "# TODO: Load the model with *random weights*. HINT: number of class labels! HINT2: AutoConfig.from_pretrained and AutoModelForSequenceClassification.from_config\n",
        "# You'll get some warnings here, but usually they can just be ignored / are expected if you read them exactly\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"distilroberta-base\", num_labels=num_labels)\n",
        "model = AutoModelForSequenceClassification.from_config(config)\n",
        "\n",
        "# TODO: Initialize the `Trainer` and start training! Don't forget passing the `compute_metrics` method!\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments,\n",
        "    train_dataset=encoded_ds[\"train\"],\n",
        "    eval_dataset=encoded_ds[\"val\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# TODO: Final evaluation after training\n",
        "\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HOrWD2g1ULWa",
      "metadata": {
        "id": "HOrWD2g1ULWa"
      },
      "outputs": [],
      "source": [
        "# Answer: AutoModelForSequenceClassification vs. AutoModel\n",
        "\n",
        "# AutoModelForSequenceClassification is, as the name says, specifically designed and optimized for sequence classification. It has a classification head on top of the encoder that maps the encoded sequence representation to the number of output classes. The activation function used in the head, for example softmax, helps with the clasification task."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "pqA3GMCuh2Yg",
      "metadata": {
        "id": "pqA3GMCuh2Yg"
      },
      "source": [
        "# Transfer Learning: Finetuning a pretrained model\n",
        "\n",
        "Nowadays, barely anyone trains a Transformer model from scratch for NLP. Instead, we initialize our model with **pretrained weights**. Usually these weights have been trained with massive amounts of data and compute and publicly released by big players like Google, Meta, Microsoft, etc... or SAP ;).\n",
        "\n",
        "HuggingFace `transformers` makes loading pretrained weights very easy with the `AutoModelFor<TaskDescription>.from_pretrained(<model-name>)` method.\n",
        "\n",
        "**Task 9 (5 points)**:\n",
        "* Load `distilroberta-base` with **pretrained weights** and finetune the model on our task for 5 epochs. Set a different `output_dir` than for the previous training. Otherwise, use the same `TrainingArguments` as before.\n",
        "* Which weights of the model were initialized from pretrained weights and which weights were still randomly initialized?\n",
        "* Briefly describe the differences you observe to training from scratch. What might be the reasons for these differences? (roughly 3-6 sentences).\n",
        "\n",
        "\n",
        "Again, you do not have to hit specific performance goals here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "tNuYGX1_z1S9",
      "metadata": {
        "id": "tNuYGX1_z1S9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing BertForSequenceClassification: ['roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.0.output.dense.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'lm_head.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.key.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['encoder.layer.4.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'classifier.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.0.attention.self.query.bias', 'classifier.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/jerome/anaconda3/envs/umlia23/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='565' max='565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [565/565 01:25, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.387412</td>\n",
              "      <td>0.160794</td>\n",
              "      <td>0.331817</td>\n",
              "      <td>0.187244</td>\n",
              "      <td>0.267500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.209580</td>\n",
              "      <td>0.630941</td>\n",
              "      <td>0.528605</td>\n",
              "      <td>0.544544</td>\n",
              "      <td>0.545000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.889476</td>\n",
              "      <td>0.655870</td>\n",
              "      <td>0.664619</td>\n",
              "      <td>0.652405</td>\n",
              "      <td>0.647500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.801108</td>\n",
              "      <td>0.727384</td>\n",
              "      <td>0.693891</td>\n",
              "      <td>0.691628</td>\n",
              "      <td>0.685000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.091500</td>\n",
              "      <td>0.743776</td>\n",
              "      <td>0.723812</td>\n",
              "      <td>0.730235</td>\n",
              "      <td>0.723373</td>\n",
              "      <td>0.717500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jerome/anaconda3/envs/umlia23/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 05:29]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.7437756061553955,\n",
              " 'eval_precision': 0.7238121024281942,\n",
              " 'eval_recall': 0.7302346486654681,\n",
              " 'eval_f1': 0.7233728751380412,\n",
              " 'eval_accuracy': 0.7175,\n",
              " 'eval_runtime': 6.1758,\n",
              " 'eval_samples_per_second': 64.769,\n",
              " 'eval_steps_per_second': 2.105,\n",
              " 'epoch': 5.0}"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer, BertForSequenceClassification  # hint\n",
        "\n",
        "# TODO: Create `TrainingArguments`\n",
        "\n",
        "TrainingArguments = TrainingArguments(\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-5,\n",
        "    output_dir=\"./results_pretrained\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    seed=1944,\n",
        "    num_train_epochs=5,\n",
        ")\n",
        "\n",
        "# TODO: Load the model with pretrained weights \n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"distilroberta-base\", num_labels=num_labels)\n",
        "\n",
        "# TODO: Initialize the `Trainer` and start training!\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments,\n",
        "    train_dataset=encoded_ds[\"train\"],\n",
        "    eval_dataset=encoded_ds[\"val\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# TODO: Final evaluation after training\n",
        "\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cHAoRrJDVLdC",
      "metadata": {
        "id": "cHAoRrJDVLdC"
      },
      "outputs": [],
      "source": [
        "# Text Answers\n",
        "# randomly initialized weights: the weights of the classification\n",
        "# pretrained weights: the weights of the encoding\n",
        "\n",
        "# Using pretrained weights we start (and end) with higher accuracy than training from scratch. This might be because the pretrained weights already contain a lot of information about the language which was probably learned on a big number of texts. Still, we need to train the classification head so that the model learns to classify the texts correctly for this specific task."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "jIAUW1JSrySK",
      "metadata": {
        "id": "jIAUW1JSrySK"
      },
      "source": [
        "# Testing\n",
        "\n",
        "**Task 10 (1 point)**: Evaluate the trained model from Task 9 (finetuning a pretrained model) on the test set. Is there a performance gap between validation and test set? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "95lxeX3oz7lV",
      "metadata": {
        "id": "95lxeX3oz7lV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.7198035717010498,\n",
              " 'eval_precision': 0.7220683785668043,\n",
              " 'eval_recall': 0.7213157894736841,\n",
              " 'eval_f1': 0.719821669580131,\n",
              " 'eval_accuracy': 0.7213157894736842,\n",
              " 'eval_runtime': 13.323,\n",
              " 'eval_samples_per_second': 570.442,\n",
              " 'eval_steps_per_second': 17.864,\n",
              " 'epoch': 5.0}"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: evaluate on testset\n",
        "\n",
        "trainer.evaluate(encoded_ds[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X9ugiCcKf9R4",
      "metadata": {
        "id": "X9ugiCcKf9R4"
      },
      "outputs": [],
      "source": [
        "# Answer: Is there a performance gap between validation and test set?\n",
        "\n",
        "# Indeed there is a performance gap between the validation and test set, and the model performs better on the latter. This might stem from the fact that all the way in the beginning we realized that the validation set specifically was not balanced. Maybe the model has an easier time with a more equal distribution of class labels."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
